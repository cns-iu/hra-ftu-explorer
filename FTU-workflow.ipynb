{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213248b9",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "1. FTU segmentation\n",
    "2. Compare FTU's cell type information between Azimuth, PopV and Celltypist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ebbdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896dc6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import filters, measure, color, morphology\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load image\n",
    "image_path = \"D:\\\\vostro\\\\Degree\\\\Masters\\\\CNS_Project\\\\Release_4\\\\issue-231\\\\PNG\\\\2d-ftu-small-intestine-villus.png\"\n",
    "image = cv2.imread(image_path)\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Gaussian Blur\n",
    "blurred = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "# Edge detection\n",
    "edges = cv2.Canny(blurred, 500, 150)\n",
    "\n",
    "# Thresholding to create a binary image\n",
    "thresh = filters.threshold_otsu(blurred)\n",
    "binary = gray_image > thresh\n",
    "\n",
    "# Remove small objects\n",
    "cleaned = morphology.remove_small_objects(binary, min_size=500)\n",
    "\n",
    "# Label the image\n",
    "labeled = measure.label(cleaned)\n",
    "image_label_overlay = color.label2rgb(labeled, image=image, bg_label=0)\n",
    "\n",
    "# Count the number of unique labels (cells)\n",
    "num_cells = np.max(labeled)\n",
    "\n",
    "# Plot the images\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5))  # Adjust subplot size\n",
    "ax[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title('Original Image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(edges, cmap='gray')\n",
    "ax[1].set_title('Edges')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(image_label_overlay)\n",
    "ax[2].set_title('Labeled Cells')\n",
    "ax[2].axis('off')\n",
    "\n",
    "# Display counts on a new subplot\n",
    "ax[3].text(0.5, 0.5, f'Total Cells: {num_cells}', horizontalalignment='center', verticalalignment='center', fontsize=12, color='red')\n",
    "ax[3].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import measure, color\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def segment_and_count_cells(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Enhance contrast using CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    contrast_enhanced = clahe.apply(gray_image)\n",
    "\n",
    "    # Apply Gaussian Blur to smooth out the image\n",
    "    blurred = cv2.GaussianBlur(contrast_enhanced, (5, 5), 0)\n",
    "\n",
    "    # Adaptive Thresholding to create a binary image\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "    # Morphological operations to clean the image\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "    # Create sure background area by dilating the opening\n",
    "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "    # Finding sure foreground area using the distance transform and thresholding\n",
    "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "    _, sure_fg = cv2.threshold(dist_transform, 0.2 * dist_transform.max(), 255, 0)\n",
    "\n",
    "    # Finding unknown region (borders)\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "    # Label markers for Watershed\n",
    "    _, markers = cv2.connectedComponents(sure_fg)\n",
    "    markers = markers + 1\n",
    "    markers[unknown == 255] = 0\n",
    "\n",
    "    # Apply the Watershed algorithm\n",
    "    markers = cv2.watershed(image, markers)\n",
    "    image[markers == -1] = [255, 0, 0]  # Mark boundaries in red\n",
    "\n",
    "    # Count cells\n",
    "    unique_markers = np.unique(markers)\n",
    "    cell_count = len(unique_markers) - 1  # exclude background\n",
    "\n",
    "    # Display results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(color.label2rgb(markers, image=image, bg_label=0), cmap='jet')\n",
    "    plt.title(f'Segmented Image - Cells Counted: {cell_count}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return cell_count\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"D:\\\\vostro\\\\Degree\\\\Masters\\\\CNS_Project\\\\Release_4\\\\issue-231\\\\PNG\\\\2d-ftu-kidney-renal-corpuscle.png\"\n",
    "total_cells = segment_and_count_cells(image_path)\n",
    "print(f\"Total cells counted: {total_cells}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c450b3",
   "metadata": {},
   "source": [
    "### Get the Cell annotation tool for cell types in FTUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Function to fetch and extract data from JSON link\n",
    "def fetch_json_data(url):\n",
    "    response = requests.get(url)\n",
    "    json_data = response.json()\n",
    "    return json_data\n",
    "\n",
    "# Load the CSV\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Supriya\\\\Downloads\\\\Query-23.csv\")\n",
    "\n",
    "# Prepare columns for 'origin', 'dataset_type', and 'dataset_info'\n",
    "data['origin'] = ''\n",
    "data['dataset_type'] = ''\n",
    "data['dataset_info'] = ''\n",
    "\n",
    "# Iterate over the rows and fetch JSON data\n",
    "for index, row in data.iterrows():\n",
    "    print(f\"Processing dataset {index+1}/{len(data)}: {row['dataset']}\")\n",
    "    try:\n",
    "        json_data = fetch_json_data(row['dataset'])\n",
    "\n",
    "        # Extract required fields\n",
    "        origins = [d['origin'] for d in json_data.get('ingest_metadata', {}).get('dag_provenance_list', [])]\n",
    "        data.at[index, 'origin'] = ', '.join(origins)\n",
    "        data.at[index, 'dataset_type'] = json_data.get('dataset_type', '')\n",
    "        data.at[index, 'dataset_info'] = json_data.get('dataset_info', '')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process dataset at {row['dataset']} due to an error: {e}\")\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "data.to_csv('C:\\\\Users\\\\Supriya\\\\Downloads\\\\updated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ab415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load JSON data\n",
    "with open(\"C:\\\\Users\\\\Supriya\\\\Downloads\\\\crosswalks.jsonld\", 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(json_data.keys())\n",
    "# Assuming json_data is structured as a dictionary with necessary keys\n",
    "json_df = pd.DataFrame(json_data['@graph'])\n",
    "\n",
    "# Convert the provided cell ID list into a DataFrame\n",
    "cell_ids_list = [\n",
    "    \"CL:1001107\", \"CL:1001111\", \"CL:1000768\", \"CL:1001106\", \"CL:0002306\",\n",
    "    \"CL:1000850\", \"CL:1000742\", \"CL:1000850\", \"CL:0000653\", \"CL:0002306\",\n",
    "    \"CL:1001005\", \"CL:1001099\", \"CL:1000452\", \"CL:1001096\", \"CL:1000714\",\n",
    "    \"CL:4030021\", \"CL:4030020\", \"CL:1001033\", \"CL:1000718\", \"CL:1001033\",\n",
    "    \"CL:1000717\", \"CL:1000716\", \"CL:1001033\", \"CL:1001107\", \"CL:1001131\",\n",
    "    \"CL:1001111\", \"CL:1001285\", \"CL:1001107\", \"CL:1001131\", \"CL:4033024\",\n",
    "    \"CL:4033022\", \"CL:4033023\", \"CL:0000313\", \"CL:4033003\", \"CL:0002063\",\n",
    "    \"CL:0009089\", \"CL:0002062\", \"CL:4028004\", \"CL:4033016\", \"CL:4028002\",\n",
    "    \"CL:1001433\", \"CL:0002064\", \"CL:0002080\", \"CL:0019032\", \"CL:1000322\",\n",
    "    \"CL:1001433\", \"CL:0002080\", \"CL:1000321\", \"CL:0002071\", \"CL:0000165\",\n",
    "    \"CL:0009016\", \"CL:0019032\", \"CL:0000171\", \"CL:0000169\", \"CL:0000173\",\n",
    "    \"CL:0002275\", \"CL:0002144\", \"CL:0000235\", \"CL:0000115\", \"CL:0000649\",\n",
    "    \"CL:0002457\", \"CL:0000646\", \"CL:0000242\", \"CL:1000458\", \"CL:0009100\",\n",
    "    \"CL:0019018\", \"CL:1000413\", \"CL:0000241\", \"CL:0000075\", \"CL:0002543\",\n",
    "    \"CL:1000398\", \"CL:0000632\", \"CL:0000091\", \"CL:0000182\", \"CL:1000301\",\n",
    "    \"CL:0000669\", \"CL:1000487\", \"CL:1000299\", \"CL:2000059\", \"CL:0002340\",\n",
    "    \"CL:0002341\", \"CL:0000359\", \"CL:0000809\", \"CL:0000942\", \"CL:0000883\",\n",
    "    \"CL:0009077\", \"CL:0001070\", \"CL:0009071\", \"CL:0000071\", \"CL:0000787\",\n",
    "    \"CL:0009070\", \"CL:0000942\", \"CL:0000738\", \"CL:0000882\", \"CL:0000788\",\n",
    "    \"CL:0001055\", \"CL:0000442\", \"CL:0000980\", \"CL:0000775\", \"CL:0000877\",\n",
    "    \"CL:0000872\", \"CL:0000874\", \"CL:0001056\", \"CL:0000037\", \"CL:0001055\",\n",
    "    \"CL:0000442\", \"CL:0000980\", \"CL:0000775\", \"CL:0000876\", \"CL:0000192\",\n",
    "    \"CL:2000053\", \"CL:0000499\", \"CL:1000489\", \"CL:0000623\", \"CL:0000057\",\n",
    "    \"CL:0009080\", \"CL:0000584\", \"CL:0000097\", \"CL:1000324\", \"CL:0000186\",\n",
    "    \"CL:0000786\", \"CL:0000451\", \"CL:0000236\", \"CL:0000771\", \"CL:0000235\",\n",
    "    \"CL:0001065\", \"CL:0000163\"\n",
    "]\n",
    "\n",
    "# Create DataFrame for the cell IDs\n",
    "cell_ids_df = pd.DataFrame(cell_ids_list, columns=['CT ID in CL'])\n",
    "\n",
    "# Match these IDs with the JSON DataFrame we previously extracted and parsed\n",
    "matched_df = cell_ids_df.merge(json_df, left_on='CT ID in CL', right_on='cell_id', how='left')\n",
    "\n",
    "print(matched_df.head())\n",
    "\n",
    "# Create new columns for 'azimuth', 'popv', and 'celltypist'\n",
    "# Populate 'azimuth', 'popv', and 'celltypist' based on the 'tool' from the JSON\n",
    "# Here, we assume any valid 'tool' entry means a match (adjust as necessary based on actual tool names)\n",
    "matched_df['azimuth'] = matched_df['tool'].apply(lambda x: 1 if x == 'azimuth' else 0)\n",
    "matched_df['popv'] = matched_df['tool'].apply(lambda x: 1 if x == 'popv' else 0)\n",
    "matched_df['celltypist'] = matched_df['tool'].apply(lambda x: 1 if x == 'celltypist' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant columns for the final CSV\n",
    "final_output_df = matched_df[['organ_level', 'organ_id', 'cell_label', 'CT ID in CL', 'azimuth', 'popv', 'celltypist']].fillna(0)\n",
    "\n",
    "print(final_output_df.head())\n",
    "\n",
    "# Show the first few rows of the final DataFrame to verify the content before saving it\n",
    "output_csv_path = 'C:\\\\Users\\\\Supriya\\\\Downloads\\\\Matched_Cell_IDs.csv'\n",
    "final_output_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24afc97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files from the uploaded paths\n",
    "azimuth_df = pd.read_csv('C:\\\\Users\\\\Supriya\\\\Downloads\\\\azimuth.csv')\n",
    "popv_df = pd.read_csv('C:\\\\Users\\\\Supriya\\\\Downloads\\\\popv.csv')\n",
    "celltypist_df = pd.read_csv('C:\\\\Users\\\\Supriya\\\\Downloads\\\\celltypist.csv')\n",
    "ftu_cell_count_df = pd.read_csv('C:\\\\Users\\\\Supriya\\\\Downloads\\\\FTU Cell Count Table - Cell_Type_Count.csv')\n",
    "\n",
    "# Display the first few rows of each dataframe to inspect their structure\n",
    "print(\"Azimuth DataFrame Sample:\")\n",
    "print(azimuth_df.head())\n",
    "\n",
    "print(\"\\nPOPV DataFrame Sample:\")\n",
    "print(popv_df.head())\n",
    "\n",
    "print(\"\\nCelltypist DataFrame Sample:\")\n",
    "print(celltypist_df.head())\n",
    "\n",
    "print(\"\\nFTU Cell Count DataFrame Sample:\")\n",
    "print(ftu_cell_count_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9822d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract unique levels from the organ_level column\n",
    "def extract_levels(df, tool):\n",
    "    levels = df['Organ_Level'].str.split('_').apply(lambda x: x[-1] if isinstance(x, list) else None).unique()\n",
    "    if tool == 'popv':\n",
    "        levels_with_suffix = [f'{tool}_{level}' for level in levels if level and 'L' in level]\n",
    "        has_suffix = df['Organ_Level'].str.contains('L', na=False).any()\n",
    "        return levels_with_suffix, has_suffix\n",
    "    return [f'{tool}_{level}' for level in levels if level], True\n",
    "\n",
    "azimuth_levels, _ = extract_levels(azimuth_df, 'azimuth')\n",
    "popv_levels, popv_has_suffix = extract_levels(popv_df, 'popv')\n",
    "celltypist_levels, _ = extract_levels(celltypist_df, 'celltypist')\n",
    "\n",
    "\n",
    "# Print the extracted levels for debugging\n",
    "print(\"\\nAzimuth Levels:\", azimuth_levels)\n",
    "print(\"\\nPOPV Levels:\", popv_levels)\n",
    "print(\"\\nCelltypist Levels:\", celltypist_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c80c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'popv' if no _Lx suffix is found in any organ_level for popv\n",
    "if not popv_has_suffix:\n",
    "    popv_levels.append('popv')\n",
    "\n",
    "# Combine all levels into a set to avoid duplicates\n",
    "all_levels = set(azimuth_levels + popv_levels + celltypist_levels)\n",
    "\n",
    "# Initialize the result dataframe with the fixed columns\n",
    "result_df = pd.DataFrame(columns=['Organ', 'FTU Label in Uberon', 'FTU ID in Uberon', 'CL_id'] + list(all_levels))\n",
    "\n",
    "# Iterate through the rows of the FTU Cell Count Table\n",
    "for _, row in ftu_cell_count_df.iterrows():\n",
    "    organ = row['Organ']\n",
    "    ftu_label = row['FTU Label in Uberon']\n",
    "    ftu_id = row['FTU ID in Uberon']\n",
    "    cell_id = row['CT ID in CL']\n",
    "\n",
    "    # Find matching organ_level in azimuth, popv, celltypist\n",
    "    azimuth_row = azimuth_df[azimuth_df['Organ_Level'].str.contains(organ, na=False)]\n",
    "    popv_row = popv_df[popv_df['Organ_Level'].str.contains(organ, na=False)]\n",
    "    celltypist_row = celltypist_df[celltypist_df['Organ_Level'].str.contains(organ, na=False)]\n",
    "\n",
    "    # Check if cell_id matches in azimuth, popv, celltypist\n",
    "    azimuth_cell = azimuth_row[azimuth_row['CL_ID'] == cell_id]\n",
    "    popv_cell = popv_row[popv_row['CL_ID'] == cell_id]\n",
    "    celltypist_cell = celltypist_row[celltypist_row['CL_ID'] == cell_id]\n",
    "\n",
    "    # Create a new row dictionary with all levels set to 0\n",
    "    new_row = {level: 0 for level in all_levels}\n",
    "    new_row.update({\n",
    "        'Organ': organ,\n",
    "        'FTU Label in Uberon': ftu_label,\n",
    "        'FTU ID in Uberon': ftu_id,\n",
    "        'CL_id': cell_id\n",
    "    })\n",
    "\n",
    "    # Populate 0 or 1 based on match\n",
    "    for level in azimuth_levels:\n",
    "        if not azimuth_cell.empty and any(azimuth_cell['Organ_Level'].str.contains(level.split('_')[-1])):\n",
    "            new_row[level] = 1\n",
    "    for level in popv_levels:\n",
    "        if level == 'popv':\n",
    "            if not popv_cell.empty and all(~popv_cell['Organ_Level'].str.contains('_')):\n",
    "                new_row[level] = 1\n",
    "        else:\n",
    "            if not popv_cell.empty and any(popv_cell['Organ_Level'].str.contains(level.split('_')[-1])):\n",
    "                new_row[level] = 1\n",
    "    for level in celltypist_levels:\n",
    "        if not celltypist_cell.empty and any(celltypist_cell['Organ_Level'].str.contains(level.split('_')[-1])):\n",
    "            new_row[level] = 1\n",
    "\n",
    "    # Append the row to the result dataframe using pd.concat\n",
    "    result_df = pd.concat([result_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "# Save the result dataframe to a CSV file\n",
    "result_path = 'C:\\\\Users\\\\Supriya\\\\Downloads\\\\combined_results_NEW.csv'\n",
    "result_df.to_csv(result_path, index=False)\n",
    "\n",
    "# Return the result dataframe for inspection\n",
    "result_df.head() #, result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b1727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CL_IDs in each dataframe\n",
    "azimuth_cl_ids = set(azimuth_df['CL_ID'])\n",
    "celltypist_cl_ids = set(celltypist_df['CL_ID'])\n",
    "popv_cl_ids = set(popv_df['CL_ID'])\n",
    "ftu_cl_ids = set(ftu_cell_count_df['CT ID in CL'])\n",
    "\n",
    "# Find the common CL_IDs in each dataframe with ftu_cell_count_df\n",
    "common_azimuth = ftu_cl_ids.intersection(azimuth_cl_ids)\n",
    "common_celltypist = ftu_cl_ids.intersection(celltypist_cl_ids)\n",
    "common_popv = ftu_cl_ids.intersection(popv_cl_ids)\n",
    "\n",
    "# Print the number of common CL_IDs in each case\n",
    "num_common_azimuth = len(common_azimuth)\n",
    "num_common_celltypist = len(common_celltypist)\n",
    "num_common_popv = len(common_popv)\n",
    "\n",
    "print(f\"Number of common CL_IDs in azimuth: {num_common_azimuth}\")\n",
    "print(f\"Number of common CL_IDs in celltypist: {num_common_celltypist}\")\n",
    "print(f\"Number of common CL_IDs in popv: {num_common_popv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c48006a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to C:\\Users\\Supriya\\Downloads\\FTU_Cell_Count_Annotated_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Supriya\\AppData\\Local\\Temp\\ipykernel_24344\\1881189952.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ftu_cell_count_filtered.loc[:, 'Organ_ID'] = ftu_cell_count_filtered['Organ'].map(organ_to_uberon)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided files\n",
    "popv = pd.read_csv('C:\\\\Users\\\\Supriya\\\\Downloads\\\\popv.csv')\n",
    "celltypist = pd.read_csv('C:\\\\Users\\\\Supriya\\\\Downloads\\\\celltypist.csv')\n",
    "azimuth = pd.read_csv('C:\\\\Users\\\\Supriya\\\\Downloads\\\\azimuth.csv')\n",
    "ftu_cell_count = pd.read_csv('C:\\\\Users\\\\Supriya\\\\Downloads\\\\FTU Cell Count Table - Cell_Type_Count.csv')\n",
    "\n",
    "# Define the mapping dictionary\n",
    "organ_to_uberon = {\n",
    "    'Kidney': 'UBERON:0002113',\n",
    "    'Lung': 'UBERON:0002048',\n",
    "    'Pancreas': 'UBERON:0001264',\n",
    "    'Large Intestine': 'UBERON:0002107',\n",
    "    'Skin': 'UBERON:0002097',\n",
    "    'Liver': 'UBERON:0002108',\n",
    "    'Prostate': 'UBERON:0002367',\n",
    "    'Thymus': 'UBERON:0002371',\n",
    "    'Spleen': 'UBERON:0002370',\n",
    "    'Small Intestine': 'UBERON:0002106'\n",
    "}\n",
    "\n",
    "# Filter FTU cell count to include only rows where `CT ID in CL` contains `CL`\n",
    "ftu_cell_count_filtered = ftu_cell_count[ftu_cell_count['CT ID in CL'].str.contains('CL', na=False)]\n",
    "\n",
    "# Map the organ names in FTU cell count table to UBERON IDs\n",
    "ftu_cell_count_filtered.loc[:, 'Organ_ID'] = ftu_cell_count_filtered['Organ'].map(organ_to_uberon)\n",
    "\n",
    "# Prepare a common structure to hold the results\n",
    "result = []\n",
    "\n",
    "# Iterate over the filtered FTU cell count data\n",
    "for idx, ftu_row in ftu_cell_count_filtered.iterrows():\n",
    "    organ = ftu_row['Organ_ID']\n",
    "    ftu_label = ftu_row['FTU Label in Uberon']\n",
    "    ftu_id = ftu_row['FTU ID in Uberon']\n",
    "    cl_id = ftu_row['CT ID in CL']\n",
    "    cl_label = ftu_row['CT Label in CL']\n",
    "    \n",
    "    # Initialize match columns\n",
    "    match_info = {\n",
    "        'Organ': ftu_row['Organ'],\n",
    "        'FTU Label in Uberon': ftu_label,\n",
    "        'FTU ID in Uberon': ftu_id,\n",
    "        'CL_id': cl_id,\n",
    "        'CT Label in CL': cl_label,\n",
    "        'popv': 0,\n",
    "        'celltypist_L1': 0,\n",
    "        'celltypist_pkl': 0,\n",
    "        'azimuth_L1': 0,\n",
    "        'azimuth_L2': 0,\n",
    "        'azimuth_L3': 0,\n",
    "        'azimuth_L4': 0,\n",
    "        'azimuth_L5': 0,\n",
    "        'azimuth_level' : 0\n",
    "    }\n",
    "    \n",
    "    # Check in celltypist\n",
    "    celltypist_matches = celltypist[(celltypist['CL_ID'] == cl_id) & (celltypist['Organ_ID'] == organ)]\n",
    "    for _, ct_match in celltypist_matches.iterrows():\n",
    "        level = ct_match['Organ_Level'].split('_')[-1]  # Extract the level number\n",
    "        match_info[f'celltypist_{level}'] = 1\n",
    "\n",
    "    # Check in azimuth\n",
    "    azimuth_matches = azimuth[(azimuth['CL_ID'] == cl_id) & (azimuth['Organ_ID'] == organ)]\n",
    "    for _, az_match in azimuth_matches.iterrows():\n",
    "        level = az_match['Organ_Level'].split('_')[-1]  # Extract the level number\n",
    "        match_info[f'azimuth_{level}'] = 1\n",
    "\n",
    "    # Check in popv\n",
    "    popv_matches = popv[(popv['CL_ID'] == cl_id) & (popv['Organ_ID'] == organ)]\n",
    "    if not popv_matches.empty:\n",
    "        match_info['popv'] = 1\n",
    "\n",
    "    # Append to results\n",
    "    result.append(match_info)\n",
    "\n",
    "# Create a dataframe from the results\n",
    "result_df = pd.DataFrame(result)\n",
    "\n",
    "# Save the result to a CSV file\n",
    "output_path = 'C:\\\\Users\\\\Supriya\\\\Downloads\\\\FTU_Cell_Count_Annotated_1.csv'\n",
    "result_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"File saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba18064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON-LD file manually to inspect its structure\n",
    "with open(\"C:\\\\Users\\\\Supriya\\\\Downloads\\\\atlas-as-cell-summaries.jsonld\", 'r') as file:\n",
    "    atlas_as_cell_summaries = json.load(file)\n",
    "\n",
    "# Convert the relevant part of the JSON-LD file to a DataFrame\n",
    "atlas_as_cell_summaries_graph = atlas_as_cell_summaries['@graph']\n",
    "atlas_as_cell_summaries_df = pd.json_normalize(atlas_as_cell_summaries_graph, 'summary', ['sex', 'cell_source_label', 'annotation_method', 'aggregated_summaries'])\n",
    "\n",
    "# Load the FTU Cell Count Table\n",
    "ftu_cell_count_table = pd.read_csv(\"C:\\\\Users\\\\Supriya\\\\Downloads\\\\FTU Cell Count Table - Cell_Type_Count.csv\")\n",
    "\n",
    "# Extract relevant columns\n",
    "ftu_relevant_columns = ftu_cell_count_table[['CT ID in CL', 'Organ']]\n",
    "atlas_relevant_columns = atlas_as_cell_summaries_df[['cell_id', 'cell_label', 'annotation_method', 'cell_source_label', 'sex', 'aggregated_summaries']]\n",
    "\n",
    "# Merge dataframes on the 'cell_id' and 'CT ID in CL' assuming they are the same\n",
    "merged_df = pd.merge(atlas_relevant_columns, ftu_relevant_columns, left_on='cell_id', right_on='CT ID in CL', how='inner')\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv(\"C:\\\\Users\\\\Supriya\\\\Downloads\\\\merged_dataframe_1.csv\", index=False)\n",
    "\n",
    "# Find cell_labels that share the same values in aggregated_summaries\n",
    "shared_summary_cells = atlas_relevant_columns.explode('aggregated_summaries')\n",
    "shared_summary_cells = shared_summary_cells.groupby('aggregated_summaries')['cell_label'].unique().reset_index()\n",
    "\n",
    "# Filter for rows where more than one cell_label share the same aggregated_summary\n",
    "shared_summary_cells = shared_summary_cells[shared_summary_cells['cell_label'].apply(lambda x: len(x) > 1)]\n",
    "\n",
    "# Modify the shared_summary_cells DataFrame to include cell_source_label\n",
    "shared_summary_cells_with_source = shared_summary_cells.explode('cell_label')\n",
    "shared_summary_cells_with_source = pd.merge(\n",
    "    shared_summary_cells_with_source,\n",
    "    atlas_as_cell_summaries_df[['cell_label', 'cell_source_label']],\n",
    "    on='cell_label',\n",
    "    how='left'\n",
    ").drop_duplicates()\n",
    "\n",
    "# Saving the shared_summary_cells_with_source DataFrame to a CSV file\n",
    "shared_summary_cells_with_source.to_csv(\"C:\\\\Users\\\\Supriya\\\\Downloads\\\\shared_summary_cells_with_source_1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2ef0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
